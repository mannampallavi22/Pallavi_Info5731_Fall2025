{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mannampallavi22/Pallavi_Info5731_Fall2025/blob/main/Mannam_Pallavi_Assignment_2_2_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed798508-aee1-4343-9350-902e7f09eb5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         asin           name  rating               date  verified  \\\n",
            "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
            "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
            "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
            "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
            "4  B0000SX2UC  tristazbimmer       4    August 28, 2005     False   \n",
            "\n",
            "                                          title  \\\n",
            "0                   Def not best, but not worst   \n",
            "1                   Text Messaging Doesn't Work   \n",
            "2                               Love This Phone   \n",
            "3                       Love the Phone, BUT...!   \n",
            "4  Great phone service and options, lousy case!   \n",
            "\n",
            "                                                body  helpfulVotes  \n",
            "0  I had the Samsung A600 for awhile which is abs...           1.0  \n",
            "1  Due to a software issue between Nokia and Spri...          17.0  \n",
            "2  This is a great, reliable phone. I also purcha...           5.0  \n",
            "3  I love the phone and all, because I really did...           1.0  \n",
            "4  The phone has been great for every purpose it ...           1.0  \n",
            "Index(['asin', 'name', 'rating', 'date', 'verified', 'title', 'body',\n",
            "       'helpfulVotes'],\n",
            "      dtype='object')\n",
            "Selected Product ID: B00F2SKPIM\n",
            "Only 981 reviews available for this product.\n",
            "Saved 981 reviews for product B00F2SKPIM into product_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the reviews dataset\n",
        "reviews = pd.read_csv(\"20191226-reviews.csv\")\n",
        "\n",
        "# Check structure of the dataset\n",
        "print(reviews.head())\n",
        "print(reviews.columns)\n",
        "\n",
        "# Choose a product ID with enough reviews\n",
        "product_id = reviews['asin'].value_counts().idxmax()  # pick product with most reviews\n",
        "print(\"Selected Product ID:\", product_id)\n",
        "\n",
        "# Filter reviews for the chosen product\n",
        "product_reviews = reviews[reviews['asin'] == product_id]\n",
        "\n",
        "# Ensure at least 1000 reviews\n",
        "if len(product_reviews) >= 1000:\n",
        "    product_reviews = product_reviews.head(1000)  # take first 1000\n",
        "else:\n",
        "    print(\"Only\", len(product_reviews), \"reviews available for this product.\")\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"product_reviews.csv\"\n",
        "product_reviews.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Saved {len(product_reviews)} reviews for product {product_id} into {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2340d1-1c40-4d45-99bc-28acdce597a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text sample:\n",
            "0    B0000SX2UC\n",
            "1    B0000SX2UC\n",
            "2    B0000SX2UC\n",
            "3    B0000SX2UC\n",
            "4    B0000SX2UC\n",
            "Name: asin, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (change file if needed)\n",
        "df = pd.read_csv(\"20191226-reviews.csv\")\n",
        "\n",
        "# Pick a text column (adjust if needed)\n",
        "text_column = \"reviewText\" if \"reviewText\" in df.columns else df.columns[0]\n",
        "\n",
        "print(\"Original text sample:\")\n",
        "print(df[text_column].head(5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "df[\"no_noise\"] = df[text_column].apply(lambda x: re.sub(rf\"[{string.punctuation}]\", \" \", str(x)))\n",
        "\n",
        "print(\"\\nAfter removing punctuation & special characters:\")\n",
        "print(df[[\"no_noise\"]].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZhrWmcxDNG",
        "outputId": "4b76e592-84e9-4b96-a246-af8f16ca5838"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After removing punctuation & special characters:\n",
            "     no_noise\n",
            "0  B0000SX2UC\n",
            "1  B0000SX2UC\n",
            "2  B0000SX2UC\n",
            "3  B0000SX2UC\n",
            "4  B0000SX2UC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"no_numbers\"] = df[\"no_noise\"].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
        "\n",
        "print(\"\\nAfter removing numbers:\")\n",
        "print(df[[\"no_numbers\"]].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It0CZtBExLgj",
        "outputId": "da5caaea-6b9f-40b4-8379-97244e61f6f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After removing numbers:\n",
            "  no_numbers\n",
            "0    B SX UC\n",
            "1    B SX UC\n",
            "2    B SX UC\n",
            "3    B SX UC\n",
            "4    B SX UC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"lowercase\"] = df[\"no_numbers\"].str.lower()\n",
        "\n",
        "print(\"\\nAfter converting to lowercase:\")\n",
        "print(df[[\"lowercase\"]].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAeG18VbxRm4",
        "outputId": "54e8d17f-80c6-422f-be96-afe622fc5a4e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After converting to lowercase:\n",
            "  lowercase\n",
            "0   b sx uc\n",
            "1   b sx uc\n",
            "2   b sx uc\n",
            "3   b sx uc\n",
            "4   b sx uc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set([\n",
        "    \"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"as\",\n",
        "    \"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\n",
        "    \"did\",\"do\",\"does\",\"doing\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\n",
        "    \"has\",\"have\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\n",
        "    \"i\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"itself\",\"just\",\"me\",\"more\",\"most\",\"my\",\"myself\",\n",
        "    \"no\",\"nor\",\"not\",\"now\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\n",
        "    \"ourselves\",\"out\",\"over\",\"own\",\"same\",\"she\",\"should\",\"so\",\"some\",\"such\",\"t\",\"than\",\n",
        "    \"that\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\n",
        "    \"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"we\",\"were\",\"what\",\"when\",\n",
        "    \"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"you\",\"your\",\"yours\",\"yourself\",\n",
        "    \"yourselves\"\n",
        "])\n",
        "\n",
        "df[\"no_stopwords\"] = df[\"lowercase\"].apply(\n",
        "    lambda x: \" \".join([w for w in x.split() if w not in stop_words])\n",
        ")\n",
        "\n",
        "print(\"\\nAfter removing stopwords:\")\n",
        "print(df[[\"no_stopwords\"]].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwQyC0EzxYOH",
        "outputId": "c39ab886-ee2a-4cfc-ccd4-99b89f3628bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After removing stopwords:\n",
            "  no_stopwords\n",
            "0      b sx uc\n",
            "1      b sx uc\n",
            "2      b sx uc\n",
            "3      b sx uc\n",
            "4      b sx uc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "df[\"stemmed\"] = df[\"no_stopwords\"].apply(\n",
        "    lambda x: \" \".join([stemmer.stem(w) for w in x.split()])\n",
        ")\n",
        "\n",
        "print(\"\\nAfter stemming:\")\n",
        "print(df[[\"stemmed\"]].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNpjczEOxrl5",
        "outputId": "1cf6388a-974d-463a-a2ca-6479d7de0667"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After stemming:\n",
            "   stemmed\n",
            "0  b sx uc\n",
            "1  b sx uc\n",
            "2  b sx uc\n",
            "3  b sx uc\n",
            "4  b sx uc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "try:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df[\"lemmatized\"] = df[\"stemmed\"].apply(\n",
        "        lambda x: \" \".join([lemmatizer.lemmatize(w) for w in x.split()])\n",
        "    )\n",
        "except:\n",
        "    df[\"lemmatized\"] = df[\"stemmed\"]  # fallback if wordnet not available\n",
        "\n",
        "print(\"\\nAfter lemmatization:\")\n",
        "print(df[[\"lemmatized\"]].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXSIoGqqxzlm",
        "outputId": "5313fac7-c5fa-4a3a-db85-436dad624fd0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After lemmatization:\n",
            "  lemmatized\n",
            "0    b sx uc\n",
            "1    b sx uc\n",
            "2    b sx uc\n",
            "3    b sx uc\n",
            "4    b sx uc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"cleaned_reviews_stepwise.csv\", index=False)\n",
        "print(\"\\n All steps completed. Final file saved as cleaned_reviews_stepwise.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEpq5hvlx_W2",
        "outputId": "d661e818-51d0-4160-fc3f-59bf57bd0f7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " All steps completed. Final file saved as cleaned_reviews_stepwise.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# ===============================\n",
        "# Load spaCy model (English)\n",
        "# ===============================\n",
        "# Run once: python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ===============================\n",
        "# Load cleaned text\n",
        "# ===============================\n",
        "df = pd.read_csv(\"cleaned_reviews_stepwise.csv\")\n",
        "\n",
        "# Adjust if your cleaned column has a different name\n",
        "text_column = \"lemmatized\" if \"lemmatized\" in df.columns else df.columns[-1]\n",
        "\n",
        "print(\"Sample cleaned text:\")\n",
        "print(df[text_column].head(3))\n",
        "\n",
        "# ===============================\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "# ===============================\n",
        "all_pos = []\n",
        "\n",
        "for doc in nlp.pipe(df[text_column].dropna().astype(str).tolist(), disable=[\"parser\",\"ner\"]):\n",
        "    for token in doc:\n",
        "        all_pos.append(token.pos_)\n",
        "\n",
        "pos_counts = Counter(all_pos)\n",
        "\n",
        "print(\"\\n=== POS Counts ===\")\n",
        "print(\"Nouns (N):\", pos_counts.get(\"NOUN\", 0))\n",
        "print(\"Verbs (V):\", pos_counts.get(\"VERB\", 0))\n",
        "print(\"Adjectives (Adj):\", pos_counts.get(\"ADJ\", 0))\n",
        "print(\"Adverbs (Adv):\", pos_counts.get(\"ADV\", 0))\n",
        "\n",
        "# ===============================\n",
        "# (2) Constituency Parsing & Dependency Parsing\n",
        "# ===============================\n",
        "# spaCy does NOT provide constituency parsing directly.\n",
        "# We’ll use spaCy for dependency parsing and give an example of constituency via nltk/ benepar if needed.\n",
        "\n",
        "example_sentence = df[text_column].dropna().astype(str).iloc[0]\n",
        "doc = nlp(example_sentence)\n",
        "\n",
        "print(\"\\n=== Dependency Parsing (Example Sentence) ===\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:10} --> {token.dep_:15} (head: {token.head.text})\")\n",
        "\n",
        "# Explanation Example\n",
        "print(\"\\nExample sentence:\")\n",
        "print(example_sentence)\n",
        "print(\"\\nUnderstanding:\")\n",
        "print(\"Dependency parsing shows how words relate to each other, e.g., subject -> verb -> object.\")\n",
        "\n",
        "# OPTIONAL Constituency Parsing (requires benepar)\n",
        "# import benepar\n",
        "# benepar.download('benepar_en3')\n",
        "# nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "# for sent in doc.sents:\n",
        "#     print(sent._.parse_string)\n",
        "\n",
        "# ===============================\n",
        "# (3) Named Entity Recognition (NER)\n",
        "# ===============================\n",
        "all_ents = []\n",
        "\n",
        "for doc in nlp.pipe(df[text_column].dropna().astype(str).tolist(), disable=[\"tagger\",\"parser\"]):\n",
        "    for ent in doc.ents:\n",
        "        all_ents.append((ent.text, ent.label_))\n",
        "\n",
        "ent_counts = Counter([label for _, label in all_ents])\n",
        "\n",
        "print(\"\\n=== Named Entity Recognition (NER) ===\")\n",
        "print(\"Entity counts by type:\")\n",
        "for ent, count in ent_counts.items():\n",
        "    print(ent, \":\", count)\n",
        "\n",
        "print(\"\\nExamples of extracted entities:\")\n",
        "print(all_ents[:10])  # first 10 entities found\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diq6oeQ6vtt7",
        "outputId": "f4c40a93-7b29-4125-f03a-7a3f6adc3f32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample cleaned text:\n",
            "0    b sx uc\n",
            "1    b sx uc\n",
            "2    b sx uc\n",
            "Name: lemmatized, dtype: object\n",
            "\n",
            "=== POS Counts ===\n",
            "Nouns (N): 51538\n",
            "Verbs (V): 2289\n",
            "Adjectives (Adj): 1563\n",
            "Adverbs (Adv): 616\n",
            "\n",
            "=== Dependency Parsing (Example Sentence) ===\n",
            "b          --> meta            (head: sx)\n",
            "sx         --> advmod          (head: uc)\n",
            "uc         --> ROOT            (head: uc)\n",
            "\n",
            "Example sentence:\n",
            "b sx uc\n",
            "\n",
            "Understanding:\n",
            "Dependency parsing shows how words relate to each other, e.g., subject -> verb -> object.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Named Entity Recognition (NER) ===\n",
            "Entity counts by type:\n",
            "ORG : 5524\n",
            "PERSON : 5840\n",
            "CARDINAL : 17\n",
            "GPE : 216\n",
            "TIME : 49\n",
            "\n",
            "Examples of extracted entities:\n",
            "[('b n l k', 'ORG'), ('b n l k', 'ORG'), ('b n l k', 'ORG'), ('b n l k', 'ORG'), ('b n l k', 'ORG'), ('b n l k', 'ORG'), ('b n l k', 'ORG'), ('mk vl', 'PERSON'), ('mk vl', 'PERSON'), ('mk vl', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://github.com/marketplace?type=actions&page=\"\n",
        "\n",
        "# List to collect data\n",
        "data = []\n",
        "\n",
        "# Scrape up to 50 pages * 20 products/page ≈ 1000 products\n",
        "for page in range(1, 51):\n",
        "    print(f\"Scraping page {page}...\")\n",
        "    url = base_url + str(page)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find all product cards\n",
        "        products = soup.find_all(\"div\", class_=\"d-flex flex-column flex-justify-between border color-border-muted rounded-2 p-3 height-full\")\n",
        "\n",
        "        for prod in products:\n",
        "            # Extract product name\n",
        "            name_tag = prod.find(\"h3\")\n",
        "            name = name_tag.get_text(strip=True) if name_tag else \"N/A\"\n",
        "\n",
        "            # Extract description\n",
        "            desc_tag = prod.find(\"p\")\n",
        "            desc = desc_tag.get_text(strip=True) if desc_tag else \"N/A\"\n",
        "\n",
        "            # Extract product URL\n",
        "            link_tag = prod.find(\"a\", href=True)\n",
        "            url = \"https://github.com\" + link_tag['href'] if link_tag else \"N/A\"\n",
        "\n",
        "            data.append([name, desc, url, page])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on page {page}: {e}\")\n",
        "\n",
        "    # Delay between requests\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(data, columns=[\"name\", \"description\", \"url\", \"page\"])\n",
        "df.to_csv(\"github_marketplace_actions.csv\", index=False)\n",
        "\n",
        "print(\"Scraping completed. Data saved to github_marketplace_actions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nok2fKUwag1",
        "outputId": "ece1fb15-6ad9-4ac6-9fbf-09ed71ce7cc0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping completed. Data saved to github_marketplace_actions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 Web Scrapping the tweets\n",
        "# Install Tweepy\n",
        "!pip install tweepy\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your actual Bearer Token from Twitter Developer Portal\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAA354QEAAAAAkulO6gTOlueM3MLrhYtGV2xeQlY%3DXqkiyhoiOPoAfCrwZbc29PVLW2xF9lD0Phbq93255FRNR84nrA\"\n",
        "\n",
        "# Authenticate with Tweepy v2\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# Define query and parameters\n",
        "query = \"#machinelearning OR #artificialintelligence -is:retweet lang:en\"\n",
        "max_results = 100\n",
        "\n",
        "# Fetch tweets\n",
        "response = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    max_results=max_results,\n",
        "    tweet_fields=[\"id\", \"text\", \"author_id\"]\n",
        ")\n",
        "\n",
        "# Extract data\n",
        "tweets_data = []\n",
        "if response.data:\n",
        "    for tweet in response.data:\n",
        "        tweets_data.append({\n",
        "            \"Tweet ID\": tweet.id,\n",
        "            \"Username\": tweet.author_id,\n",
        "            \"Text\": tweet.text\n",
        "        })\n",
        "\n",
        "# Save raw data\n",
        "df_raw = pd.DataFrame(tweets_data)\n",
        "df_raw.to_csv(\"raw_tweets.csv\", index=False)\n",
        "print(f\"✅ Scraped {len(df_raw)} tweets.\")\n",
        "print(df_raw.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hFAnJO_YsVo",
        "outputId": "bac8b31a-5323-4463-be68-22bb083c8daa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.8.3)\n",
            "✅ Scraped 100 tweets.\n",
            "              Tweet ID             Username  \\\n",
            "0  1972846917022625953  1227549819310243841   \n",
            "1  1972846270831296997  1246719394027311104   \n",
            "2  1972846164300140775  1967977988755980288   \n",
            "3  1972846111628169546  1249601030502756353   \n",
            "4  1972845351985103334            952502046   \n",
            "\n",
            "                                                Text  \n",
            "0  RT @GeostatsGuy: After a lecture on principal ...  \n",
            "1  RT @CervantesGarces: 🔥#AHORA #BigData #Hadoop ...  \n",
            "2  Just explored the MLOps workflow with MLflow 🚀...  \n",
            "3  RT @jblefevre60: 5 project to learn AI!\\n\\n#AI...  \n",
            "4  For creative minds https://t.co/9ilgTaZESd\\nPl...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 2 Data Cleaning Procedure\n",
        "import re\n",
        "\n",
        "# Load raw data\n",
        "df = pd.read_csv(\"raw_tweets.csv\")\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Cleaned Text\"] = df[\"Text\"].apply(clean_text)\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "df.drop_duplicates(subset=\"Tweet ID\", inplace=True)\n",
        "df.dropna(subset=[\"Tweet ID\", \"Username\", \"Cleaned Text\"], inplace=True)\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv(\"cleaned_tweets.csv\", index=False)\n",
        "print(f\"\\n✅ Cleaned data saved. Final record count: {len(df)}\")\n",
        "print(df[[\"Tweet ID\", \"Username\", \"Cleaned Text\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kp6BJXHYtBa",
        "outputId": "b13db9a0-f35c-422e-8dfe-2f83368c6470"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Cleaned data saved. Final record count: 100\n",
            "              Tweet ID             Username  \\\n",
            "0  1972846917022625953  1227549819310243841   \n",
            "1  1972846270831296997  1246719394027311104   \n",
            "2  1972846164300140775  1967977988755980288   \n",
            "3  1972846111628169546  1249601030502756353   \n",
            "4  1972845351985103334            952502046   \n",
            "\n",
            "                                        Cleaned Text  \n",
            "0  RT  After a lecture on principal component ana...  \n",
            "1                       RT       \\n\\n270925 12PMGMT5  \n",
            "2  Just explored the MLOps workflow with MLflow \\...  \n",
            "3                          RT  5 project to learn AI  \n",
            "4  For creative minds \\nPlay inside of your  with...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}